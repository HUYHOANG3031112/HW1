{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self, init_theta=None, alpha=0.01, n_iter=100):\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        self.theta = init_theta\n",
    "        self.JHist = None\n",
    "\n",
    "    def gradientDescent(self, X, y, theta):\n",
    "        n, d = X.shape\n",
    "        self.JHist = []\n",
    "        for i in range(self.n_iter):\n",
    "            self.JHist.append((self.computeCost(X, y, theta), theta))\n",
    "            print(\"Iteration:\", i + 1, \" Cost:\", self.JHist[i][0], \" Theta:\", theta)\n",
    "            # Update equation\n",
    "            theta = theta - (self.alpha / n) * (X.T @ (X @ theta - y))\n",
    "        return theta\n",
    "\n",
    "    def computeCost(self, X, y, theta):\n",
    "        n = len(y)\n",
    "        # Objective (cost) equation\n",
    "        cost = (1 / (2 * n)) * np.sum(np.square(X @ theta - y))\n",
    "        return cost.item()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(y)\n",
    "        _, d = X.shape\n",
    "        if self.theta is None:\n",
    "            self.theta = np.zeros((d, 1))\n",
    "        self.theta = self.gradientDescent(X, y, self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Prediction function\n",
    "        predictions = X @ self.theta\n",
    "        return predictions.flatten()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
